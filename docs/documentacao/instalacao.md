# Instala√ß√£o

O **Data Pipeline Project** √© uma solu√ß√£o moderna que utiliza ferramentas como **Airflow**, **DBT**, **Jupyter** e **Superset** para orquestra√ß√£o, transforma√ß√£o, an√°lise e visualiza√ß√£o de dados. Este guia ajudar√° voc√™ a come√ßar rapidamente.

---

## üìã Pr√©-requisitos

Antes de come√ßar, certifique-se de ter os seguintes softwares instalados:

- **Docker e Docker Compose**: Para gerenciamento de cont√™ineres.
- **Make**: Ferramenta de automa√ß√£o de build.
- **Python 3.x**: Para execu√ß√£o de scripts e desenvolvimento.
- **Git**: Controle de vers√£o.

Caso precise de ajuda para instalar esses componentes, consulte a documenta√ß√£o oficial de cada ferramenta:

- [Instala√ß√£o do Docker](https://docs.docker.com/get-docker/)
- [Guia do Python](https://www.python.org/downloads/)
- [Guia do Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)

---

## üöÄ Instala√ß√£o

### 1. Clonando o Reposit√≥rio

Para obter o c√≥digo-fonte do projeto, clone o reposit√≥rio Git:

```bash
git clone git@gitlab.com:lappis-unb/gest-odadosipea/app-lappis-ipea.git
cd app-lappis-ipea
```

### 2. Configurando o Ambiente

Execute o comando abaixo para configurar automaticamente o ambiente de desenvolvimento:

```bash
make setup
```

Este comando ir√°:

- Criar ambientes virtuais necess√°rios.
- Instalar depend√™ncias do projeto.
- Configurar hooks de pr√©-commit.
- Preparar o ambiente de desenvolvimento para execu√ß√£o local.

!!! note "Dica" Caso encontre problemas durante a configura√ß√£o, verifique se o Docker est√° rodando corretamente e se voc√™ possui permiss√µes administrativas no sistema.

## üèÉ‚Äç‚ôÇÔ∏è Executando o Projeto Localmente

Ap√≥s a configura√ß√£o, inicialize todos os servi√ßos com o Docker Compose:

```bash
docker-compose up -d
```

### Acessando os Componentes

Uma vez que os servi√ßos estejam em execu√ß√£o, voc√™ pode acessar as ferramentas principais nos seguintes URLs:

- Airflow: http://localhost:8080
- Jupyter: http://localhost:8888
- Superset: http://localhost:8088

Certifique-se de que todas as portas mencionadas estejam dispon√≠veis no seu ambiente.

## üõ† Estrutura do Projeto

A estrutura do projeto √© organizada para separar cada componente da stack, facilitando a manuten√ß√£o e o desenvolvimento:

```bash
.
‚îú‚îÄ‚îÄ airflow_lappis
‚îÇ   ‚îú‚îÄ‚îÄ dags
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_ingest
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ compras_gov
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ siafi
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ siape
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ siorg
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tesouro_gerencial
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transfere_gov
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dbt
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ipea
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ cosmos_dag.py
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ dbt_project.yml
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ descriptions.yml
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ macros
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ models
‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ contratos_dbt
‚îÇ   ‚îÇ           ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bronze
‚îÇ   ‚îÇ           ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ silver
‚îÇ   ‚îÇ           ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gold
‚îÇ   ‚îÇ           ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ views
‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ orcamento_dbt
‚îÇ   ‚îÇ           ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ bronze
‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ pessoas_dbt
‚îÇ   ‚îÇ           ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bronze
‚îÇ   ‚îÇ           ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ silver
‚îÇ   ‚îÇ           ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gold
‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ schema.yml
‚îÇ   ‚îÇ           ‚îÇ   ‚îú‚îÄ‚îÄ sources.yml
‚îÇ   ‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ ted_dbt
‚îÇ   ‚îÇ           ‚îÇ       ‚îú‚îÄ‚îÄ bronze
‚îÇ   ‚îÇ           ‚îÇ       ‚îú‚îÄ‚îÄ silver
‚îÇ   ‚îÇ           ‚îÇ       ‚îú‚îÄ‚îÄ gold
‚îÇ   ‚îÇ           ‚îÇ       ‚îî‚îÄ‚îÄ views
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ profiles.yml
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ snapshots
‚îÇ   ‚îú‚îÄ‚îÄ helpers
‚îÇ   ‚îú‚îÄ‚îÄ plugins
‚îÇ   ‚îî‚îÄ‚îÄ templates
‚îú‚îÄ‚îÄ docker
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ Makefile
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt

```

Essa organiza√ß√£o modular permite que cada componente seja desenvolvido e mantido de forma independente.

---

## üéØ Comandos √öteis no Makefile

O **Makefile** facilita a execu√ß√£o de tarefas repetitivas e a configura√ß√£o do ambiente. Aqui est√£o os principais comandos dispon√≠veis:

#### `make setup`

> **Prepara o ambiente do projeto.**
> Instala todas as depend√™ncias do projeto definidas no `pyproject.toml`, incluindo as depend√™ncias de desenvolvimento. Tamb√©m exporta essas depend√™ncias para um arquivo `requirements.txt` (√∫til para ambientes como Docker ou CI/CD) e executa um script de configura√ß√£o de *git hooks*.

---

#### `make format`

> **Formata o c√≥digo automaticamente.**
> Executa ferramentas de formata√ß√£o para padronizar o estilo do c√≥digo:

* [`black`](https://black.readthedocs.io/) para c√≥digo Python
* [`ruff`](https://docs.astral.sh/ruff/) para corre√ß√µes r√°pidas
* [`sqlfmt`](https://sqlfmt.com/) para formatar scripts SQL localizados na pasta `airflow_lappis/dags/dbt`

---

#### `make lint`

> **Verifica a qualidade do c√≥digo.**
> Executa valida√ß√µes de estilo e qualidade est√°tica:

* Verifica se o c√≥digo est√° corretamente formatado com `black` (`--check`)
* Analisa problemas com `ruff` (sem corrigir)
* Executa `mypy` para checar tipos est√°ticos
* Valida formata√ß√£o SQL com `sqlfmt`
* Roda o `sqlfluff` (caso n√£o esteja em ambiente CI) para valida√ß√µes avan√ßadas de SQL

---

#### `make test`

> **Executa os testes automatizados.**
> Roda os testes presentes na pasta `tests/` usando o framework [`pytest`](https://docs.pytest.org/).


---

# Teste (airflow, dbt)

Este passo a passo descreve o passo a passo para configurar e executar o pipeline completo, desde a ingest√£o de dados no **Airflow** at√© o tratamento no **dbt**.

---


## 1. Configurar Airflow

### 1.1 Configurar vari√°veis de ambiente do Airflow

- Acesse o airflow:

Airflow: http://localhost:8080


- Ap√≥s subir os containers via Docker (`docker compose up -d`), √© necess√°rio configurar as vari√°veis de ambiente no **Airflow ‚Üí Admin ‚Üí Variables**.

![Fluxo de Dados](../../assets/images/variables.png)

- Clique em "+" para adicionar uma nova vari√°vel de ambiente

![Fluxo de Dados](../../assets/images/add_variable.png)


- Adicione as duas Key & Value, uma de cada vez, e salve-as:

<details>
  <summary>1- Key & Value</summary>

  <pre>Key: <code>
    airflow_orgao
  </code></pre>
  
  <pre>Value: <code>
    ipea
  </code></pre>

</details>

<details>
  <summary>2- Key & Value</summary>

  <pre>Key: <code>
    airflow_variables
  </code></pre>
  <pre>Value: <code>{
    "ipea": {
      "codigos_ug": [113601, 113602]
    },
    "unb": {
      "codigos_ug": [154040]
    },
    "ibama": {
      "codigos_ug": [440001, 440048, 440050]
    },
    "mgi": {
      "codigos_ug": [201082]
    }
  }</code></pre>
</details>
<br><br>

### 1.2 Configurar banco local com o Airflow

- Clique em connections:

![Fluxo de Dados](../../assets/images/connections.png)<br><br>

- Busque pela conex√£o pr√© configurada do postgres e clique em edit record:

![Fluxo de Dados](../../assets/images/banco_postgres.png) <br><br>

- Altere apenas Host, Database, Login, Password e Porta

```bash
HOST=localhost
DBNAME=postgres
USER=postgres
PASSWORD=postgres
PORT=5432
``` 

- Clique em `Test` para testar a conex√£o com o banco e depois salve!

## 2. Rodar a DAG de contratos

No painel do Airflow:

1. Localize a DAG `api_contratos_dag`.
2. Ative a DAG clicando no bot√£o de "play"(‚ñ∂Ô∏è) - Trigger DAG.
3. Aparecer√° a cor verde escuro, indicando sucesso ao rodar a DAG.

Essa DAG far√° a ingest√£o dos dados de contratos a partir das fontes configuradas.

---

## 3. Validar a ingest√£o no banco de dados

Ap√≥s a execu√ß√£o da DAG, conecte-se ao banco de dados Postgres para validar se as tabelas de contratos foram populadas.

As credenciais do banco est√£o definidas no arquivo **`.env`** do reposit√≥rio:

```dotenv
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_DB=postgres
HOST=localhost
```

A porta padr√£o exposta no Docker √© **5432**.
Comando de conex√£o (exemplo via `psql`):

```bash
psql -h localhost -U postgres -d postgres
```

---

## 4. Ajustar a configura√ß√£o do dbt

Antes de rodar os modelos do dbt, √© necess√°rio garantir que os arquivos de configura√ß√£o apontem para o banco **postgres**(local) e n√£o mais para **analytics**(produ√ß√£o).

### a) Arquivo `profiles.yml`

Deve estar assim:

```yaml
ipea:
  target: prod
  outputs:
    prod:
      type: postgres
      host: localhost
      user: postgres
      password: postgres
      port: 5432
      dbname: postgres
      schema: ipea
```

### b) Arquivo `dbt_project.yml`

Altere a linha onde aparece `+database: analytics` para:

```yaml
+database: postgres
```

### c) Arquivos de snapshots

Nos arquivos de snapshot (`tables_snapshot.yml`), troque todos os `database: analytics` por `database: postgres`.

<details>
  <summary>Resultado arquivo snapshots</summary>

  <pre>tables_snapshot.yml <code>
  snapshots:
    - name: contratos_snapshot
      relation: ref('contratos')
      config:
        schema: snapshots
        database: postgres
        unique_key: id
        strategy: check
        check_cols: [situacao, num_parcelas, valor_parcela, valor_global, valor_acumulado]

    - name: faturas_snapshot
      relation: ref('faturas')
      config:
        schema: snapshots
        database: postgres
        unique_key: [id, id_empenho]
        strategy: check
        check_cols: [situacao, valor, juros, multa, glosa]

    - name: empenhos_snapshot
      relation: ref('empenhos')
      config:
        schema: snapshots
        database: postgres
        unique_key: [id, contrato_id]
        strategy: check
        check_cols: [empenhado, aliquidar, liquidado, pago, rpinscrito, rpaliquidar, rpliquidado, rppago]

    - name: cronogramas_snapshot
      relation: ref('cronogramas')
      config:
        schema: snapshots
        database: postgres
        unique_key: id
        strategy: check
        check_cols: [valor, retroativo, observacao]
  </code></pre>
</details>


---

## 5. Testar conex√£o do dbt com o banco

No diret√≥rio do projeto dbt, rode:

```bash
dbt debug
```

Voc√™ deve ver no log algo como:

```
Connection:
  host: localhost
  port: 5432
  user: postgres
  database: postgres
  schema: ipea
  Connection test: OK connection ok
```

---

## 6. Rodar o modelo de contratos no dbt

Agora rode o modelo `contratos` para iniciar o fluxo de tratamento dos dados da camada **raw ‚Üí bronze**:

```bash
dbt run -m contratos
```

Esse comando executa apenas o modelo `contratos.sql`, respons√°vel por transformar os dados brutos em uma tabela organizada na camada bronze.

---

## ‚úÖ Conclus√£o

Seguindo estes passos, voc√™ ter√°:

1. Configurado o Airflow com vari√°veis de ambiente corretas.
2. Executado a DAG de ingest√£o de contratos.
3. Validado a ingest√£o no banco Postgres.
4. Ajustado o dbt para rodar em `postgres` (em vez de `analytics`).
5. Rodado o modelo `contratos` para iniciar o tratamento dos dados.



## üìö Documenta√ß√£o √ötil
Para aproveitar ao m√°ximo os componentes do projeto, consulte as documenta√ß√µes oficiais:

- [Documenta√ß√£o do Airflow](https://airflow.apache.org/docs/)
- [Documenta√ß√£o do DBT](https://docs.getdbt.com/)
- [Documenta√ß√£o do Superset](https://superset.apache.org/docs/intro)
